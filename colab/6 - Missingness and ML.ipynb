{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvQYUhOqtN0d"
      },
      "source": [
        "# $\\color{purple}{\\text{Understanding Missing Data and How to Deal with It (Part 6)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TopzKHmOtVtL"
      },
      "source": [
        "## $\\color{purple}{\\text{Missing Data in the Age of Machine Learning and Artifical Neural Network}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "colab"
        ]
      },
      "source": [
        "### $\\color{purple}{\\text{Colab Environmental Setup}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "colab"
        ]
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/missingness_tutorial')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "colab"
        ]
      },
      "source": [
        "%pip install autoimpute"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $\\color{purple}{\\text{Libraries for this lesson}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from autoimpute.imputations import MiceImputer\n",
        "from autoimpute.imputations import SingleImputer\n",
        "from matplotlib.patches import Rectangle\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from helpers import ImputationDisplayer, stat_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $\\color{purple}{\\text{Neural Network Imputers}}$\n",
        "\n",
        "#### $\\color{purple}{\\text{Denoising Autoencoders}}$\n",
        "\n",
        "* The missing data (or deviation from an imputed value) is treated as noise.\n",
        "* Denoising autoencoders are neural networks trained on the same input and output.\n",
        "* Theory is that the output is trained so that the output is the input with noise removed.\n",
        "* To work properly, data should be normalized during the imputation.\n",
        "\n",
        "`scaler` uses `sklearn`'s `StandardScaler`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/full_set.csv')\n",
        "dmar_df = pd.read_csv('data/double_mar_set.csv')\n",
        "ImputationDisplayer(dmar_df)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(dmar_df)\n",
        "sdmar_df = pd.DataFrame(scaler.transform(dmar_df), columns=dmar_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def restore_df(scaler, x):\n",
        "    \"\"\"\n",
        "    Inverse the scaler and created a dataframe\n",
        "    \"\"\"\n",
        "    return pd.DataFrame(scaler.inverse_transform(x), columns=dmar_df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The basic autoencoder proposed by [Gondara and Wang](https://arxiv.org/abs/1705.02737)\n",
        "![](https://raw.githubusercontent.com/WestHealth/scipy2022-missingness-tutorial/main/images/dae.svg)\n",
        "* Deep neural network with 5 hidden layers with a dropout layer\n",
        "* $\\Theta$ is a hyperparameter governing the expansion and contraction of the layer\n",
        "* $\\Theta=7$ is suggested by best practice.\n",
        "* In the first 3 hidden layers, each layer expands by $\\Theta$ and contracts by $\\Theta$ in the last 2 hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1 Impute the data set using univariate imputation\n",
        "The recommendation is that mean or median imputation of numeric data and mode imputation of categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "univariate_imputed = SingleImputer('median').fit_transform(sdmar_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2 Split data into training and test sets\n",
        "This is only necessary if you are building a model that accepts future data (open configuration). If the data set is closed (i.e. you don't expect any new data) then you can set the test_size to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "theta = 7\n",
        "# Divide into training and test sets\n",
        "training, test = train_test_split(univariate_imputed, test_size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3 Build, Compile and Train a Deep Neural Network Model\n",
        "* theta and activation function are hyperparameters\n",
        "\n",
        "See `tensorflow` and `keras` documentation for further detail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(5 + theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5 + 2 * theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5 + 3 * theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5 + 2 * theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5 + theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(training, training, epochs=50, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can visualize the progress of the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4 Make Prediction based on initial imputation.\n",
        "We replace the missing values with the predicted value. We also convert back to `pandas` `DataFrame`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = pd.DataFrame(model.predict(univariate_imputed),\n",
        "                         columns=dmar_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Don't forget to rescale the data after filling in missing data\n",
        "imputed = restore_df(scaler, sdmar_df.combine_first(predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "master"
        ]
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "master"
        ]
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "master"
        ]
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $\\color{purple}{\\text{Improved Feedback Denoising Autoencoders}}$\n",
        "\n",
        "My own enhancement to the denoising autoencoder see [here](https://arxiv.org/abs/2002.08338)\n",
        "\n",
        "The algorithm was designed for closed data sets. This example shows one enhancement to the denoising autoencoder (DAE), the iterative refinement of the imputed values. It starts similarly by univariate imputation as **step 1**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "univariate_imputation = SingleImputer('median').fit_transform(sdmar_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2 Build and Compile Deep Neural Network Model\n",
        "We use the same architecture as the DAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(5 + theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5 + 2 * theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5 + 3 * theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5 + 2 * theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5 + theta, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(5)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3 Initial Fit\n",
        "Fewer epochs than standard DAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = [\n",
        "    model.fit(univariate_imputation,\n",
        "              univariate_imputation,\n",
        "              epochs=10,\n",
        "              verbose=False)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4 Iteration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = pd.DataFrame(model.predict(univariate_imputation),\n",
        "                         columns=dmar_df.columns)\n",
        "iterated_imputation = sdmar_df.combine_first(predicted)\n",
        "history.append(\n",
        "    model.fit(iterated_imputation,\n",
        "              iterated_imputation,\n",
        "              epochs=2,\n",
        "              verbose=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Repeat the Iteration a Prescribed Number of Times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(0, 19):\n",
        "    predicted = pd.DataFrame(model.predict(iterated_imputation),\n",
        "                             columns=dmar_df.columns)\n",
        "    iterated_imputation = sdmar_df.combine_first(predicted)\n",
        "    history.append(\n",
        "        model.fit(iterated_imputation,\n",
        "                  iterated_imputation,\n",
        "                  epochs=2,\n",
        "                  verbose=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Since we collected history in several batches, concatenate them so we can see a plot\n",
        "losses = sum([each.history['loss'] for each in history], [])\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plug the final prediction into the missing values and rescale the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = pd.DataFrame(model.predict(iterated_imputation),\n",
        "                         columns=dmar_df.columns)\n",
        "imputed = restore_df(scaler, sdmar_df.combine_first(predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dmar_df.displayer(imputed, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "master"
        ]
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILSdXRpPtiU8"
      },
      "source": [
        "## $\\color{purple}{\\text{How Imputation Fits Into Your Machine Learning Models}}$\n",
        "\n",
        "* Typical ML Workflow\n",
        "  * Train\n",
        "  * Test\n",
        "  * Use\n",
        "  \n",
        "![](https://raw.githubusercontent.com/WestHealth/scipy2022-missingness-tutorial/main/images/typical.svg)\n",
        "\n",
        "* Save Your Pipeline\n",
        "* You can include an imputer in your pipeline\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/WestHealth/scipy2022-missingness-tutorial/main/images/imputer.svg)\n",
        "\n",
        "* We demonstrate this using `sklearn`'s pipeline. But this is meant to describe abstractly what you should do\n",
        "\n",
        "Same data set is taken from the [Wine Quality Dataset at UCI](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\n",
        "\n",
        "This demonstrates a typical pipeline. The final column `quality` is the predicted value. The `features` variable contains all the other column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AcjuKGBZBDN"
      },
      "outputs": [],
      "source": [
        "training = pd.read_csv('data/original_wine_training.csv')\n",
        "test = pd.read_csv('data/original_wine_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "master"
        ]
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = list(training.columns[0:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We build the pipeline by one-hot encoding the `type` column which is categorical, then scale it, then apply random forest regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n",
        "    ColumnTransformer([(\"type\", OneHotEncoder(), [\"type\"])],\n",
        "                      remainder='passthrough'), StandardScaler(),\n",
        "    RandomForestRegressor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.fit(training[features], training['quality'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.score(test[features], test['quality'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.predict(training[features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZZt4mf7mpd6"
      },
      "source": [
        "### $\\color{purple}{\\text{Imputer in the Data Pipeline}}$\n",
        "\n",
        "This is meant to demonstrate workflow and `autoimpute` is used as an example.\n",
        "\n",
        "One drawback is that `autoimpute` imputers require a `pandas` `DataFrame` as an input so custom transformers need to be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pandas_hack = FunctionTransformer(\n",
        "    lambda x: pd.DataFrame(x, columns=['type_r', 'type_w'] + features[0:-1]))\n",
        "pandas_hack_full = FunctionTransformer(\n",
        "    lambda x: pd.DataFrame(x, columns=['type_r', 'type_w'] + features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can insert the imputer into the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n",
        "    ColumnTransformer([(\"type\", OneHotEncoder(), ['type'])],\n",
        "                      remainder='passthrough'), pandas_hack,\n",
        "    SingleImputer(strategy='least squares'), StandardScaler(),\n",
        "    RandomForestRegressor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wine_training = pd.read_csv('data/wine_training.csv')\n",
        "pipeline.fit(wine_training[features], wine_training['quality'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wine_test = pd.read_csv('data/wine_test.csv')\n",
        "pipeline.score(wine_test[features], wine_test['quality'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wine_future = pd.read_csv('data/wine_future.csv')\n",
        "pipeline.predict(wine_future[features].iloc[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## $\\color{purple}{\\text{How does Multiple Imputation fit in?}}$\n",
        "### $\\color{purple}{\\text{Approach 1: Augment Data with Multiple Copies}}$\n",
        "\n",
        "![](https://raw.githubusercontent.com/WestHealth/scipy2022-missingness-tutorial/main/images/stack.svg)\n",
        "\n",
        "Augmentation teachs the model that the imputed values are \"fuzzy\" by providing different values.\n",
        "\n",
        "We create the same pipeline except we have a MiceImputer at the end.\n",
        "The resultant `dfs` are 5 copies of our dataframe with 5 separate imputations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 2640,
          "status": "ok",
          "timestamp": 1654712480068,
          "user": {
            "displayName": "Haw-minn Lu",
            "userId": "16109571175851064283"
          },
          "user_tz": 420
        },
        "id": "zwFrIcTcm87_",
        "outputId": "bf99ee06-2531-437c-988b-c1bd2d87652e"
      },
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n",
        "    ColumnTransformer([(\"type\", OneHotEncoder(), ['type'])],\n",
        "                      remainder='passthrough'), StandardScaler(), pandas_hack,\n",
        "    MiceImputer(k=5, strategy='stochastic'))\n",
        "dfs = [each[1] for each in pipeline.fit_transform(wine_training[features])]\n",
        "len(dfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdHaAdHHm38b"
      },
      "source": [
        "We augment the training set by concatenating the 5 different data frame. Equivalently, we could rotate each epoch with different imputations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_training = pd.concat(dfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsfJcQLfm9rh"
      },
      "source": [
        "Build out model as a classification problem. Bear in mind this is just for demonstration purposes, we model is not a particularly good estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quality = pd.concat([\n",
        "    wine_training.quality, wine_training.quality, wine_training.quality,\n",
        "    wine_training.quality, wine_training.quality\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8ZUY3sEjJax"
      },
      "outputs": [],
      "source": [
        "model.fit(augmented_training, quality, epochs=10, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since test set is used to evaluate when the model may hit overtraining, it is not necessary to multiply impute the tests. But you may."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dfs = [each[1] for each in pipeline.transform(wine_test[features])]\n",
        "test1 = test_dfs[0]  # Variation one, just take one imputation\n",
        "test2 = pd.concat(test_dfs)  # Variation two augment in the same way\n",
        "quality1 = wine_test.quality\n",
        "quality2 = pd.concat([quality1, quality1, quality1, quality1, quality1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "master"
        ]
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "master"
        ]
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What about future values?\n",
        "Same options:\n",
        " * take one imputation\n",
        " * run all imputations through the model and us an ensemble technique to combine (e.g., majority voting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "future_dfs = [\n",
        "    each[1].iloc[[0]] for each in pipeline.transform(wine_future[features])\n",
        "]\n",
        "future1 = future_dfs[0]  # Variation one, just take one imputation\n",
        "future2 = pd.concat(future_dfs)  # Variation two augment in the same way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variation 1: Pick First Imputed\n",
        "np.argmax(model.predict(future1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variation 2: Aggregate all Imputed\n",
        "np.argmax(model.predict(future2).sum(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $\\color{purple}{\\text{Approach 2: Combine Multiple Models}}$\n",
        "\n",
        "![](https://raw.githubusercontent.com/WestHealth/scipy2022-missingness-tutorial/main/images/ensemble.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [\n",
        "    tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
        "        tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "    ]) for _ in range(0, 5)\n",
        "]\n",
        "for model in models:\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each model is trained on a different imputation model. Each model should be tested under each imputation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model, training in zip(models, dfs):\n",
        "    model.fit(training, wine_training.quality, epochs=10, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model, test in zip(models, test_dfs):\n",
        "    print(model.evaluate(test, wine_test.quality, verbose=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np vstack turns the list of arrays into an array of arrays\n",
        "predictions = np.vstack(\n",
        "    [model.predict(future) for model, future in zip(models, future_dfs)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate predictions\n",
        "np.argmax(predictions.sum(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $\\color{purple}{\\text{Worth Mentioning: Use Single Imputation with Bagging}}$\n",
        "Short for bootstrap and aggregation\n",
        "\n",
        "Rather than multiple imputation, single imputations are performed from resampled datasets (bootstrapping)\n",
        "\n",
        "![](https://raw.githubusercontent.com/WestHealth/scipy2022-missingness-tutorial/main/images/single.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n",
        "    ColumnTransformer([(\"type\", OneHotEncoder(), ['type'])],\n",
        "                      remainder='passthrough'), StandardScaler(),\n",
        "    pandas_hack_full, MiceImputer(k=1, strategy='stochastic'))\n",
        "bagged_dfs = [\n",
        "    next(pipeline.fit_transform(wine_training.sample(frac=1, replace=True)))[1]\n",
        "    for _ in range(0, 5)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [\n",
        "    tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
        "        tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "    ]) for _ in range(0, 5)\n",
        "]\n",
        "for model in models:\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model, training in zip(models, dfs):\n",
        "    model.fit(training, wine_training.quality, epochs=10, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = np.vstack(\n",
        "    [model.predict(future) for model, future in zip(models, future_dfs)])\n",
        "# Aggregate predictions\n",
        "np.argmax(predictions.sum(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $\\color{purple}{\\text{Worth Mentioning: Use missingness as a feature}}$\n",
        "\n",
        "The idea is that you are giving information to the model as to which values are imputed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(\n",
        "    ColumnTransformer([(\"type\", OneHotEncoder(), ['type'])],\n",
        "                      remainder='passthrough'), StandardScaler(), pandas_hack,\n",
        "    SingleImputer(strategy='stochastic'))\n",
        "processed = pipeline.fit_transform(wine_test[features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add a feature for each feature that has missing values indicating whether the corresponding row entry is missing that feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for each in features:\n",
        "    processed[f'{each}_missing'] = wine_test[each].isnull().astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## $\\color{purple}{\\text{Conclusion}}$\n",
        "\n",
        "\n",
        "* The bulk of work with models dealing with missing data uses decision tree derivative models\n",
        "* Neural Networks can be used for imputation\n",
        "* Several strategies for integrating imputation into model building pipelines\n",
        "  * Imputer should be a processing step (important that models are saveable)\n",
        "  * Multiple Imputation can use data augmentation or multiple models\n",
        "  * Bagging can be applied to single imputation performed multiply\n",
        "  * Missingness (or imputed) can be added as a flag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## $\\color{purple}{\\text{References}}$\n",
        "* Gondara, L., Wang, K.: Mida: Multiple imputation using denoising autoencoders.\n",
        "In: Phung, D., Tseng, V.S., Webb, G.I., Ho, B., Ganji, M., Rashidi, L. (eds.)\n",
        "_Advances in Knowledge Discovery and Data Mining_. pp. 260\u2013272. Springer International Publishing, Cham (2018)\n",
        "* Jiang, W., Josse, J., Lavielle, M: Logistic regression with missing covariates\u2013parameter estimation, model selection and prediction. _Computational and Statistics Analysis_, 2019.\n",
        "* Lu, H.-m., Perrone, G., & Unpingco, J.: Multiple imputation with denoising autoencoder using metamorphic truth and imputation feedback, _Machine Learning and Data Mining in Pattern Recognition, 16th International\n",
        "Conference on Machine Learning and Data Mining, MLDM 2020_,Amsterdam, The Netherlands, July 20-21, 2020, Proceedings,\n",
        "pages 197\u2013208.\n",
        "* Perez-Lebel, A., Varoquaux, G., Le Morvan, M., Josse, J., Poline, J.-B.: Benchmarking missing-values approaches for predictive models on health databases, _GigaScience_, Volume 11, 2022, https://doi.org/10.1093/gigascience/giac013\n",
        "* Khan, S., Ahmad, A., Mihailidis, A.: Bootstrapping and Multiple Imputation Ensemble Approaches for Missing Data, _Journal of Intelligent and Fuzzy Systems_, 2019. https://doi.org/10.48550/arXiv.1802.00154"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPpLv6R3L0XkmwitYmu11KS",
      "collapsed_sections": [],
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}