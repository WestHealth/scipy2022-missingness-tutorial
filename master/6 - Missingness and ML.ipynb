{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvQYUhOqtN0d"
   },
   "source": [
    "# $\\color{purple}{\\text{Understanding Missing Data and How to Deal with It (Part 6)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TopzKHmOtVtL"
   },
   "source": [
    "## $\\color{purple}{\\text{Missing Data in the Age of Machine Learning and Artifical Neural Network}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "colab"
    ]
   },
   "source": [
    "### $\\color{purple}{\\text{Colab Environmental Setup}}$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "colab"
    ]
   },
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/My Drive/missingness_tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{purple}{\\text{Libraries for this lesson}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from autoimpute.imputations import MiceImputer\n",
    "from autoimpute.imputations import SingleImputer\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from helpers import ImputationDisplayer, stat_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{purple}{\\text{Neural Network Imputers}}$\n",
    "\n",
    "#### $\\color{purple}{\\text{Denoising Autoencoders}}$\n",
    "\n",
    "* The missing data (or deviation from an imputed value) is treated as noise.\n",
    "* Denoising autoencoders are neural networks trained on the same input and output.\n",
    "* Theory is that the output is trained so that the output is the input with noise removed.\n",
    "* To work properly, data should be normalized during the imputation.\n",
    "\n",
    "`scaler` uses `sklearn`'s `StandardScaler`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/full_set.csv')\n",
    "dmar_df = pd.read_csv('data/double_mar_set.csv')\n",
    "ImputationDisplayer(dmar_df)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dmar_df)\n",
    "sdmar_df = pd.DataFrame(scaler.transform(dmar_df), columns=dmar_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_df(scaler, x):\n",
    "    \"\"\"\n",
    "    Inverse the scaler and created a dataframe\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(scaler.inverse_transform(x), columns=dmar_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic autoencoder proposed by [Gondara and Wang](https://arxiv.org/abs/1705.02737)\n",
    "* Deep neural network with 5 hidden layers with a dropout layer\n",
    "* $\\Theta$ is a hyperparameter governing the expansion and contraction of the layer\n",
    "* $\\Theta=7$ is suggested by best practice.\n",
    "* In the first 3 hidden layers, each layer expands by $\\Theta$ and contracts by $\\Theta$ in the last 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 Impute the data set using univariate imputation\n",
    "The recommendation is that mean or median imputation of numeric data and mode imputation of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_imputed = SingleImputer('median').fit_transform(sdmar_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Split data into training and test sets\n",
    "This is only necessary if you are building a model that accepts future data (open configuration). If the data set is closed (i.e. you don't expect any new data) then you can set the test_size to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 7\n",
    "# Divide into training and test sets\n",
    "training, test = train_test_split(univariate_imputed, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 Build, Compile and Train a Deep Neural Network Model\n",
    "* theta and activation function are hyperparameters\n",
    "\n",
    "See `tensorflow` and `keras` documentation for further detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 21:42:01.847637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-05 21:42:01.847721: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-05 21:42:01.847751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7501b53e19ae): /proc/driver/nvidia/version does not exist\n",
      "2022-07-05 21:42:01.848102: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(5 + theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5 + 2 * theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5 + 3 * theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5 + 2 * theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5 + theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 21:42:12.794264: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training, training, epochs=50, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the progress of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feb1850bc10>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 Make Prediction based on initial imputation.\n",
    "We replace the missing values with the predicted value. We also convert back to `pandas` `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(model.predict(univariate_imputed),\n",
    "                         columns=dmar_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to rescale the data after filling in missing data\n",
    "imputed = restore_df(scaler, sdmar_df.combine_first(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "master"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c5e7e_row0_col1, #T_c5e7e_row1_col0, #T_c5e7e_row8_col0 {\n",
       "  background-color: paleturquoise;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c5e7e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c5e7e_level0_col0\" class=\"col_heading level0 col0\" >feature a</th>\n",
       "      <th id=\"T_c5e7e_level0_col1\" class=\"col_heading level0 col1\" >feature b</th>\n",
       "      <th id=\"T_c5e7e_level0_col2\" class=\"col_heading level0 col2\" >feature c</th>\n",
       "      <th id=\"T_c5e7e_level0_col3\" class=\"col_heading level0 col3\" >feature d</th>\n",
       "      <th id=\"T_c5e7e_level0_col4\" class=\"col_heading level0 col4\" >uncorrelated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c5e7e_row0_col0\" class=\"data row0 col0\" >2.777245</td>\n",
       "      <td id=\"T_c5e7e_row0_col1\" class=\"data row0 col1\" >2.030612</td>\n",
       "      <td id=\"T_c5e7e_row0_col2\" class=\"data row0 col2\" >-1.552282</td>\n",
       "      <td id=\"T_c5e7e_row0_col3\" class=\"data row0 col3\" >8.772158</td>\n",
       "      <td id=\"T_c5e7e_row0_col4\" class=\"data row0 col4\" >0.360789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c5e7e_row1_col0\" class=\"data row1 col0\" >2.404555</td>\n",
       "      <td id=\"T_c5e7e_row1_col1\" class=\"data row1 col1\" >2.223169</td>\n",
       "      <td id=\"T_c5e7e_row1_col2\" class=\"data row1 col2\" >-0.645673</td>\n",
       "      <td id=\"T_c5e7e_row1_col3\" class=\"data row1 col3\" >8.815675</td>\n",
       "      <td id=\"T_c5e7e_row1_col4\" class=\"data row1 col4\" >0.393466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c5e7e_row2_col0\" class=\"data row2 col0\" >1.936893</td>\n",
       "      <td id=\"T_c5e7e_row2_col1\" class=\"data row2 col1\" >2.182897</td>\n",
       "      <td id=\"T_c5e7e_row2_col2\" class=\"data row2 col2\" >-2.474526</td>\n",
       "      <td id=\"T_c5e7e_row2_col3\" class=\"data row2 col3\" >8.549983</td>\n",
       "      <td id=\"T_c5e7e_row2_col4\" class=\"data row2 col4\" >0.891191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c5e7e_row3_col0\" class=\"data row3 col0\" >2.484183</td>\n",
       "      <td id=\"T_c5e7e_row3_col1\" class=\"data row3 col1\" >1.618856</td>\n",
       "      <td id=\"T_c5e7e_row3_col2\" class=\"data row3 col2\" >-1.404809</td>\n",
       "      <td id=\"T_c5e7e_row3_col3\" class=\"data row3 col3\" >7.360904</td>\n",
       "      <td id=\"T_c5e7e_row3_col4\" class=\"data row3 col4\" >0.156937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c5e7e_row4_col0\" class=\"data row4 col0\" >2.089727</td>\n",
       "      <td id=\"T_c5e7e_row4_col1\" class=\"data row4 col1\" >2.698967</td>\n",
       "      <td id=\"T_c5e7e_row4_col2\" class=\"data row4 col2\" >-1.464007</td>\n",
       "      <td id=\"T_c5e7e_row4_col3\" class=\"data row4 col3\" >7.786100</td>\n",
       "      <td id=\"T_c5e7e_row4_col4\" class=\"data row4 col4\" >0.046178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c5e7e_row5_col0\" class=\"data row5 col0\" >3.369940</td>\n",
       "      <td id=\"T_c5e7e_row5_col1\" class=\"data row5 col1\" >2.842243</td>\n",
       "      <td id=\"T_c5e7e_row5_col2\" class=\"data row5 col2\" >-0.716772</td>\n",
       "      <td id=\"T_c5e7e_row5_col3\" class=\"data row5 col3\" >8.767852</td>\n",
       "      <td id=\"T_c5e7e_row5_col4\" class=\"data row5 col4\" >0.342830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c5e7e_row6_col0\" class=\"data row6 col0\" >0.764542</td>\n",
       "      <td id=\"T_c5e7e_row6_col1\" class=\"data row6 col1\" >1.532282</td>\n",
       "      <td id=\"T_c5e7e_row6_col2\" class=\"data row6 col2\" >-3.850084</td>\n",
       "      <td id=\"T_c5e7e_row6_col3\" class=\"data row6 col3\" >7.992571</td>\n",
       "      <td id=\"T_c5e7e_row6_col4\" class=\"data row6 col4\" >0.678203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c5e7e_row7_col0\" class=\"data row7 col0\" >3.539507</td>\n",
       "      <td id=\"T_c5e7e_row7_col1\" class=\"data row7 col1\" >2.120999</td>\n",
       "      <td id=\"T_c5e7e_row7_col2\" class=\"data row7 col2\" >0.181066</td>\n",
       "      <td id=\"T_c5e7e_row7_col3\" class=\"data row7 col3\" >7.158813</td>\n",
       "      <td id=\"T_c5e7e_row7_col4\" class=\"data row7 col4\" >0.942345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c5e7e_row8_col0\" class=\"data row8 col0\" >2.391970</td>\n",
       "      <td id=\"T_c5e7e_row8_col1\" class=\"data row8 col1\" >1.816557</td>\n",
       "      <td id=\"T_c5e7e_row8_col2\" class=\"data row8 col2\" >-1.325628</td>\n",
       "      <td id=\"T_c5e7e_row8_col3\" class=\"data row8 col3\" >8.144401</td>\n",
       "      <td id=\"T_c5e7e_row8_col4\" class=\"data row8 col4\" >0.460626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5e7e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c5e7e_row9_col0\" class=\"data row9 col0\" >0.445575</td>\n",
       "      <td id=\"T_c5e7e_row9_col1\" class=\"data row9 col1\" >0.827475</td>\n",
       "      <td id=\"T_c5e7e_row9_col2\" class=\"data row9 col2\" >-4.705004</td>\n",
       "      <td id=\"T_c5e7e_row9_col3\" class=\"data row9 col3\" >8.161819</td>\n",
       "      <td id=\"T_c5e7e_row9_col4\" class=\"data row9 col4\" >0.014623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7feb183a3ac0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmar_df.displayer(imputed, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "master"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>With Missing Data</th>\n",
       "      <th>difference</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.411082</td>\n",
       "      <td>2.209132</td>\n",
       "      <td>0.201950</td>\n",
       "      <td>8.375926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>2.420924</td>\n",
       "      <td>2.302770</td>\n",
       "      <td>0.118154</td>\n",
       "      <td>4.880549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stdev</th>\n",
       "      <td>1.279456</td>\n",
       "      <td>1.128238</td>\n",
       "      <td>0.151218</td>\n",
       "      <td>11.818952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Original  With Missing Data  difference  percentage\n",
       "mean    2.411082           2.209132    0.201950    8.375926\n",
       "median  2.420924           2.302770    0.118154    4.880549\n",
       "stdev   1.279456           1.128238    0.151218   11.818952"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_comparison(df, imputed, 'feature a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "master"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>With Missing Data</th>\n",
       "      <th>difference</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.167609</td>\n",
       "      <td>2.052141</td>\n",
       "      <td>0.115468</td>\n",
       "      <td>5.326985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>2.169837</td>\n",
       "      <td>2.069987</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>4.601742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stdev</th>\n",
       "      <td>0.752618</td>\n",
       "      <td>0.666328</td>\n",
       "      <td>0.086290</td>\n",
       "      <td>11.465284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Original  With Missing Data  difference  percentage\n",
       "mean    2.167609           2.052141    0.115468    5.326985\n",
       "median  2.169837           2.069987    0.099850    4.601742\n",
       "stdev   0.752618           0.666328    0.086290   11.465284"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_comparison(df, imputed, 'feature b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{purple}{\\text{Improved Feedback Denoising Autoencoders}}$\n",
    "\n",
    "My own enhancement to the denoising autoencoder see [here](https://arxiv.org/abs/2002.08338)\n",
    "\n",
    "The algorithm was designed for closed data sets. This example shows one enhancement to the denoising autoencoder (DAE), the iterative refinement of the imputed values. It starts similarly by univariate imputation as **step 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_imputation = SingleImputer('median').fit_transform(sdmar_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Build and Compile Deep Neural Network Model\n",
    "We use the same architecture as the DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(5 + theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5 + 2 * theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5 + 3 * theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5 + 2 * theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5 + theta, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 Initial Fit\n",
    "Fewer epochs than standard DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\n",
    "    model.fit(univariate_imputation,\n",
    "              univariate_imputation,\n",
    "              epochs=10,\n",
    "              verbose=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(model.predict(univariate_imputation),\n",
    "                         columns=dmar_df.columns)\n",
    "iterated_imputation = sdmar_df.combine_first(predicted)\n",
    "history.append(\n",
    "    model.fit(iterated_imputation,\n",
    "              iterated_imputation,\n",
    "              epochs=2,\n",
    "              verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the Iteration a Prescribed Number of Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(0, 19):\n",
    "    predicted = pd.DataFrame(model.predict(iterated_imputation),\n",
    "                             columns=dmar_df.columns)\n",
    "    iterated_imputation = sdmar_df.combine_first(predicted)\n",
    "    history.append(\n",
    "        model.fit(iterated_imputation,\n",
    "                  iterated_imputation,\n",
    "                  epochs=2,\n",
    "                  verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7feb1811af40>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Since we collected history in several batches, concatenate them so we can see a plot\n",
    "losses = sum([each.history['loss'] for each in history], [])\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plug the final prediction into the missing values and rescale the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(model.predict(iterated_imputation),\n",
    "                         columns=dmar_df.columns)\n",
    "imputed = restore_df(scaler, sdmar_df.combine_first(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ff5f2_row0_col1, #T_ff5f2_row1_col0, #T_ff5f2_row8_col0 {\n",
       "  background-color: paleturquoise;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ff5f2\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ff5f2_level0_col0\" class=\"col_heading level0 col0\" >feature a</th>\n",
       "      <th id=\"T_ff5f2_level0_col1\" class=\"col_heading level0 col1\" >feature b</th>\n",
       "      <th id=\"T_ff5f2_level0_col2\" class=\"col_heading level0 col2\" >feature c</th>\n",
       "      <th id=\"T_ff5f2_level0_col3\" class=\"col_heading level0 col3\" >feature d</th>\n",
       "      <th id=\"T_ff5f2_level0_col4\" class=\"col_heading level0 col4\" >uncorrelated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ff5f2_row0_col0\" class=\"data row0 col0\" >2.777245</td>\n",
       "      <td id=\"T_ff5f2_row0_col1\" class=\"data row0 col1\" >2.203462</td>\n",
       "      <td id=\"T_ff5f2_row0_col2\" class=\"data row0 col2\" >-1.552282</td>\n",
       "      <td id=\"T_ff5f2_row0_col3\" class=\"data row0 col3\" >8.772158</td>\n",
       "      <td id=\"T_ff5f2_row0_col4\" class=\"data row0 col4\" >0.360789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ff5f2_row1_col0\" class=\"data row1 col0\" >2.532886</td>\n",
       "      <td id=\"T_ff5f2_row1_col1\" class=\"data row1 col1\" >2.223169</td>\n",
       "      <td id=\"T_ff5f2_row1_col2\" class=\"data row1 col2\" >-0.645673</td>\n",
       "      <td id=\"T_ff5f2_row1_col3\" class=\"data row1 col3\" >8.815675</td>\n",
       "      <td id=\"T_ff5f2_row1_col4\" class=\"data row1 col4\" >0.393466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ff5f2_row2_col0\" class=\"data row2 col0\" >1.936893</td>\n",
       "      <td id=\"T_ff5f2_row2_col1\" class=\"data row2 col1\" >2.182897</td>\n",
       "      <td id=\"T_ff5f2_row2_col2\" class=\"data row2 col2\" >-2.474526</td>\n",
       "      <td id=\"T_ff5f2_row2_col3\" class=\"data row2 col3\" >8.549983</td>\n",
       "      <td id=\"T_ff5f2_row2_col4\" class=\"data row2 col4\" >0.891191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ff5f2_row3_col0\" class=\"data row3 col0\" >2.484183</td>\n",
       "      <td id=\"T_ff5f2_row3_col1\" class=\"data row3 col1\" >1.618856</td>\n",
       "      <td id=\"T_ff5f2_row3_col2\" class=\"data row3 col2\" >-1.404809</td>\n",
       "      <td id=\"T_ff5f2_row3_col3\" class=\"data row3 col3\" >7.360904</td>\n",
       "      <td id=\"T_ff5f2_row3_col4\" class=\"data row3 col4\" >0.156937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ff5f2_row4_col0\" class=\"data row4 col0\" >2.089727</td>\n",
       "      <td id=\"T_ff5f2_row4_col1\" class=\"data row4 col1\" >2.698967</td>\n",
       "      <td id=\"T_ff5f2_row4_col2\" class=\"data row4 col2\" >-1.464007</td>\n",
       "      <td id=\"T_ff5f2_row4_col3\" class=\"data row4 col3\" >7.786100</td>\n",
       "      <td id=\"T_ff5f2_row4_col4\" class=\"data row4 col4\" >0.046178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_ff5f2_row5_col0\" class=\"data row5 col0\" >3.369940</td>\n",
       "      <td id=\"T_ff5f2_row5_col1\" class=\"data row5 col1\" >2.842243</td>\n",
       "      <td id=\"T_ff5f2_row5_col2\" class=\"data row5 col2\" >-0.716772</td>\n",
       "      <td id=\"T_ff5f2_row5_col3\" class=\"data row5 col3\" >8.767852</td>\n",
       "      <td id=\"T_ff5f2_row5_col4\" class=\"data row5 col4\" >0.342830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_ff5f2_row6_col0\" class=\"data row6 col0\" >0.764542</td>\n",
       "      <td id=\"T_ff5f2_row6_col1\" class=\"data row6 col1\" >1.532282</td>\n",
       "      <td id=\"T_ff5f2_row6_col2\" class=\"data row6 col2\" >-3.850084</td>\n",
       "      <td id=\"T_ff5f2_row6_col3\" class=\"data row6 col3\" >7.992571</td>\n",
       "      <td id=\"T_ff5f2_row6_col4\" class=\"data row6 col4\" >0.678203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_ff5f2_row7_col0\" class=\"data row7 col0\" >3.539507</td>\n",
       "      <td id=\"T_ff5f2_row7_col1\" class=\"data row7 col1\" >2.120999</td>\n",
       "      <td id=\"T_ff5f2_row7_col2\" class=\"data row7 col2\" >0.181066</td>\n",
       "      <td id=\"T_ff5f2_row7_col3\" class=\"data row7 col3\" >7.158813</td>\n",
       "      <td id=\"T_ff5f2_row7_col4\" class=\"data row7 col4\" >0.942345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_ff5f2_row8_col0\" class=\"data row8 col0\" >2.463634</td>\n",
       "      <td id=\"T_ff5f2_row8_col1\" class=\"data row8 col1\" >1.816557</td>\n",
       "      <td id=\"T_ff5f2_row8_col2\" class=\"data row8 col2\" >-1.325628</td>\n",
       "      <td id=\"T_ff5f2_row8_col3\" class=\"data row8 col3\" >8.144401</td>\n",
       "      <td id=\"T_ff5f2_row8_col4\" class=\"data row8 col4\" >0.460626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ff5f2_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_ff5f2_row9_col0\" class=\"data row9 col0\" >0.445575</td>\n",
       "      <td id=\"T_ff5f2_row9_col1\" class=\"data row9 col1\" >0.827475</td>\n",
       "      <td id=\"T_ff5f2_row9_col2\" class=\"data row9 col2\" >-4.705004</td>\n",
       "      <td id=\"T_ff5f2_row9_col3\" class=\"data row9 col3\" >8.161819</td>\n",
       "      <td id=\"T_ff5f2_row9_col4\" class=\"data row9 col4\" >0.014623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7feb1823e4f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmar_df.displayer(imputed, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "master"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>With Missing Data</th>\n",
       "      <th>difference</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.411082</td>\n",
       "      <td>2.228198</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>7.585142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>2.420924</td>\n",
       "      <td>2.359472</td>\n",
       "      <td>0.061452</td>\n",
       "      <td>2.538379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stdev</th>\n",
       "      <td>1.279456</td>\n",
       "      <td>1.132350</td>\n",
       "      <td>0.147106</td>\n",
       "      <td>11.497507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Original  With Missing Data  difference  percentage\n",
       "mean    2.411082           2.228198    0.182884    7.585142\n",
       "median  2.420924           2.359472    0.061452    2.538379\n",
       "stdev   1.279456           1.132350    0.147106   11.497507"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_comparison(df, imputed, 'feature a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILSdXRpPtiU8"
   },
   "source": [
    "## $\\color{purple}{\\text{How Imputation Fits Into Your Machine Learning Models}}$\n",
    "\n",
    "* Typical ML Workflow\n",
    "  * Train\n",
    "  * Test\n",
    "  * Use\n",
    "* Save Your Pipeline\n",
    "* You can include an imputer in your pipeline\n",
    "\n",
    "* We demonstrate this using `sklearn`'s pipeline. But this is meant to describe abstractly what you should do\n",
    "\n",
    "Same data set is taken from the [Wine Quality Dataset at UCI](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)\n",
    "\n",
    "This demonstrates a typical pipeline. The final column `quality` is the predicted value. The `features` variable contains all the other column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0AcjuKGBZBDN"
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv('data/original_wine_training.csv')\n",
    "test = pd.read_csv('data/original_wine_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "master"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>type</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.082</td>\n",
       "      <td>12.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.99850</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10.7</td>\n",
       "      <td>red</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.28</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.052</td>\n",
       "      <td>55.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.99620</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.9</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.123</td>\n",
       "      <td>27.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.4</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.37</td>\n",
       "      <td>16.95</td>\n",
       "      <td>0.048</td>\n",
       "      <td>43.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.99950</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>9.2</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.028</td>\n",
       "      <td>32.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.1</td>\n",
       "      <td>white</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>9.8</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.062</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.99480</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.57</td>\n",
       "      <td>11.5</td>\n",
       "      <td>red</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>8.3</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.070</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.99670</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.58</td>\n",
       "      <td>11.0</td>\n",
       "      <td>red</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.036</td>\n",
       "      <td>35.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.98936</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.44</td>\n",
       "      <td>13.5</td>\n",
       "      <td>white</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.27</td>\n",
       "      <td>15.80</td>\n",
       "      <td>0.035</td>\n",
       "      <td>46.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.2</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.039</td>\n",
       "      <td>36.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.99059</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.47</td>\n",
       "      <td>11.1</td>\n",
       "      <td>white</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               8.0             0.500         0.39            2.60      0.082   \n",
       "1               6.6             0.280         0.28            8.50      0.052   \n",
       "2               7.0             0.190         0.23            5.70      0.123   \n",
       "3               7.4             0.200         0.37           16.95      0.048   \n",
       "4               7.8             0.280         0.34            1.60      0.028   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4995            9.8             0.300         0.39            1.70      0.062   \n",
       "4996            8.3             0.845         0.01            2.20      0.070   \n",
       "4997            7.1             0.360         0.28            2.40      0.036   \n",
       "4998            6.6             0.240         0.27           15.80      0.035   \n",
       "4999            6.9             0.300         0.45            1.40      0.039   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    12.0                  46.0  0.99850  3.43       0.62   \n",
       "1                    55.0                 211.0  0.99620  3.09       0.55   \n",
       "2                    27.0                 104.0  0.99540  3.04       0.54   \n",
       "3                    43.0                 190.0  0.99950  3.03       0.42   \n",
       "4                    32.0                 118.0  0.99010  3.00       0.38   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4995                  3.0                   9.0  0.99480  3.14       0.57   \n",
       "4996                  5.0                  14.0  0.99670  3.32       0.58   \n",
       "4997                 35.0                 115.0  0.98936  3.19       0.44   \n",
       "4998                 46.0                 188.0  0.99820  3.24       0.51   \n",
       "4999                 36.0                 122.0  0.99059  3.07       0.47   \n",
       "\n",
       "      alcohol   type  quality  \n",
       "0        10.7    red        6  \n",
       "1         8.9  white        6  \n",
       "2         9.4  white        6  \n",
       "3         9.2  white        6  \n",
       "4        12.1  white        7  \n",
       "...       ...    ...      ...  \n",
       "4995     11.5    red        7  \n",
       "4996     11.0    red        4  \n",
       "4997     13.5  white        7  \n",
       "4998      9.2  white        5  \n",
       "4999     11.1  white        7  \n",
       "\n",
       "[5000 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(training.columns[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the pipeline by one-hot encoding the `type` column which is categorical, then scale it, then apply random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ColumnTransformer([(\"type\", OneHotEncoder(), [\"type\"])],\n",
    "                      remainder='passthrough'), StandardScaler(),\n",
    "    RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('type', OneHotEncoder(),\n",
       "                                                  ['type'])])),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('randomforestregressor', RandomForestRegressor())])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(training[features], training['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.541352377909254"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(test[features], test['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.96, 5.44, 5.95, ..., 6.89, 5.14, 6.73])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(training[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZZt4mf7mpd6"
   },
   "source": [
    "### $\\color{purple}{\\text{Imputer in the Data Pipeline}}$\n",
    "\n",
    "This is meant to demonstrate workflow and `autoimpute` is used as an example.\n",
    "\n",
    "One drawback is that `autoimpute` imputers require a `pandas` `DataFrame` as an input so custom transformers need to be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_hack = FunctionTransformer(\n",
    "    lambda x: pd.DataFrame(x, columns=['type_r', 'type_w'] + features[0:-1]))\n",
    "pandas_hack_full = FunctionTransformer(\n",
    "    lambda x: pd.DataFrame(x, columns=['type_r', 'type_w'] + features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can insert the imputer into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ColumnTransformer([(\"type\", OneHotEncoder(), ['type'])],\n",
    "                      remainder='passthrough'), pandas_hack,\n",
    "    SingleImputer(strategy='least squares'), StandardScaler(),\n",
    "    RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('type', OneHotEncoder(),\n",
       "                                                  ['type'])])),\n",
       "                ('functiontransformer',\n",
       "                 FunctionTransformer(func=<function <lambda> at 0x7feb180b3700>)),\n",
       "                ('singleimputer', SingleImputer(strategy='least squares')),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('randomforestregressor', RandomForestRegressor())])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_training = pd.read_csv('data/wine_training.csv')\n",
    "pipeline.fit(wine_training[features], wine_training['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47306633820528143"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_test = pd.read_csv('data/wine_test.csv')\n",
    "pipeline.score(wine_test[features], wine_test['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.79, 6.36, 5.15, 4.88, 5.47, 5.11, 4.66, 5.33, 5.45, 6.19])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_future = pd.read_csv('data/wine_future.csv')\n",
    "pipeline.predict(wine_future[features].iloc[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{purple}{\\text{How does Multiple Imputation fit in?}}$\n",
    "### $\\color{purple}{\\text{Approach 1: Augment Data with Multiple Copies}}$\n",
    "\n",
    "Augmentation teachs the model that the imputed values are \"fuzzy\" by providing different values.\n",
    "\n",
    "We create the same pipeline except we have a MiceImputer at the end.\n",
    "The resultant `dfs` are 5 copies of our dataframe with 5 separate imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2640,
     "status": "ok",
     "timestamp": 1654712480068,
     "user": {
      "displayName": "Haw-minn Lu",
      "userId": "16109571175851064283"
     },
     "user_tz": 420
    },
    "id": "zwFrIcTcm87_",
    "outputId": "bf99ee06-2531-437c-988b-c1bd2d87652e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ColumnTransformer([(\"type\", OneHotEncoder(), ['type'])],\n",
    "                      remainder='passthrough'), StandardScaler(), pandas_hack,\n",
    "    MiceImputer(k=5, strategy='stochastic'))\n",
    "dfs = [each[1] for each in pipeline.fit_transform(wine_training[features])]\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdHaAdHHm38b"
   },
   "source": [
    "We augment the training set by concatenating the 5 different data frame. Equivalently, we could rotate each epoch with different imputations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_training = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsfJcQLfm9rh"
   },
   "source": [
    "Build out model as a classification problem. Bear in mind this is just for demonstration purposes, we model is not a particularly good estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = pd.concat([\n",
    "    wine_training.quality, wine_training.quality, wine_training.quality,\n",
    "    wine_training.quality, wine_training.quality\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "e8ZUY3sEjJax"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb18248cd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(augmented_training, quality, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since test set is used to evaluate when the model may hit overtraining, it is not necessary to multiply impute the tests. But you may."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs = [each[1] for each in pipeline.transform(wine_test[features])]\n",
    "test1 = test_dfs[0]  # Variation one, just take one imputation\n",
    "test2 = pd.concat(test_dfs)  # Variation two augment in the same way\n",
    "quality1 = wine_test.quality\n",
    "quality2 = pd.concat([quality1, quality1, quality1, quality1, quality1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "master"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 1ms/step - loss: 1.0475 - accuracy: 0.5520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0474531650543213, 0.5519999861717224]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test1, quality1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "master"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 886us/step - loss: 1.0503 - accuracy: 0.5450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.050349235534668, 0.5450000166893005]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test2, quality2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about future values?\n",
    "Same options:\n",
    " * take one imputation\n",
    " * run all imputations through the model and us an ensemble technique to combine (e.g., majority voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dfs = [\n",
    "    each[1].iloc[[0]] for each in pipeline.transform(wine_future[features])\n",
    "]\n",
    "future1 = future_dfs[0]  # Variation one, just take one imputation\n",
    "future2 = pd.concat(future_dfs)  # Variation two augment in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variation 1: Pick First Imputed\n",
    "np.argmax(model.predict(future1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variation 2: Aggregate all Imputed\n",
    "np.argmax(model.predict(future2).sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{purple}{\\text{Approach 2: Combine Multiple Models}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
    "        tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ]) for _ in range(0, 5)\n",
    "]\n",
    "for model in models:\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model is trained on a different imputation model. Each model should be tested under each imputation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, training in zip(models, dfs):\n",
    "    model.fit(training, wine_training.quality, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0651652812957764, 0.5379999876022339]\n",
      "[1.0751641988754272, 0.531000018119812]\n",
      "[1.0714432001113892, 0.5350000262260437]\n",
      "[1.0796154737472534, 0.5370000004768372]\n",
      "[1.0800997018814087, 0.5080000162124634]\n"
     ]
    }
   ],
   "source": [
    "for model, test in zip(models, test_dfs):\n",
    "    print(model.evaluate(test, wine_test.quality, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13130 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7feb18457c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 13131 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7feb490e0f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# np vstack turns the list of arrays into an array of arrays\n",
    "predictions = np.vstack(\n",
    "    [model.predict(future) for model, future in zip(models, future_dfs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate predictions\n",
    "np.argmax(predictions.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{purple}{\\text{Worth Mentioning: Use Single Imputation with Bagging}}$\n",
    "Short for bootstrap and aggregation\n",
    "\n",
    "Rather than multiple imputation, single imputations are performed from resampled datasets (bootstrapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ColumnTransformer([(\"type\", OneHotEncoder(), ['type'])],\n",
    "                      remainder='passthrough'), StandardScaler(),\n",
    "    pandas_hack_full, MiceImputer(k=1, strategy='stochastic'))\n",
    "bagged_dfs = [\n",
    "    next(pipeline.fit_transform(wine_training.sample(frac=1, replace=True)))[1]\n",
    "    for _ in range(0, 5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
    "        tf.keras.layers.Dense(20, activation=tf.nn.tanh),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ]) for _ in range(0, 5)\n",
    "]\n",
    "for model in models:\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, training in zip(models, dfs):\n",
    "    model.fit(training, wine_training.quality, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.vstack(\n",
    "    [model.predict(future) for model, future in zip(models, future_dfs)])\n",
    "# Aggregate predictions\n",
    "np.argmax(predictions.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{purple}{\\text{Worth Mentioning: Use missingness as a feature}}$\n",
    "\n",
    "The idea is that you are giving information to the model as to which values are imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    ColumnTransformer([(\"type\", OneHotEncoder(), ['type'])],\n",
    "                      remainder='passthrough'), StandardScaler(), pandas_hack,\n",
    "    SingleImputer(strategy='stochastic'))\n",
    "processed = pipeline.fit_transform(wine_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a feature for each feature that has missing values indicating whether the corresponding row entry is missing that feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in features:\n",
    "    processed[f'{each}_missing'] = wine_test[each].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_r</th>\n",
       "      <th>type_w</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>...</th>\n",
       "      <th>citric acid_missing</th>\n",
       "      <th>residual sugar_missing</th>\n",
       "      <th>chlorides_missing</th>\n",
       "      <th>free sulfur dioxide_missing</th>\n",
       "      <th>total sulfur dioxide_missing</th>\n",
       "      <th>density_missing</th>\n",
       "      <th>pH_missing</th>\n",
       "      <th>sulphates_missing</th>\n",
       "      <th>alcohol_missing</th>\n",
       "      <th>type_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.718304</td>\n",
       "      <td>-1.718304</td>\n",
       "      <td>0.663555</td>\n",
       "      <td>1.448871</td>\n",
       "      <td>-0.583833</td>\n",
       "      <td>-0.568905</td>\n",
       "      <td>0.806965</td>\n",
       "      <td>-0.307492</td>\n",
       "      <td>-0.190794</td>\n",
       "      <td>0.505750</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.581969</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>-0.867392</td>\n",
       "      <td>0.544211</td>\n",
       "      <td>-0.507574</td>\n",
       "      <td>-0.811454</td>\n",
       "      <td>-0.619186</td>\n",
       "      <td>0.348643</td>\n",
       "      <td>0.142705</td>\n",
       "      <td>-1.665259</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.581969</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>0.905283</td>\n",
       "      <td>-0.860624</td>\n",
       "      <td>2.978233</td>\n",
       "      <td>-0.172977</td>\n",
       "      <td>-0.359886</td>\n",
       "      <td>0.336280</td>\n",
       "      <td>1.384973</td>\n",
       "      <td>-0.773533</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.581969</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>1.388740</td>\n",
       "      <td>-0.456462</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>1.179730</td>\n",
       "      <td>-0.846074</td>\n",
       "      <td>-0.585088</td>\n",
       "      <td>0.879915</td>\n",
       "      <td>0.697814</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.581969</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>-0.464511</td>\n",
       "      <td>-1.033836</td>\n",
       "      <td>-0.035823</td>\n",
       "      <td>-0.898027</td>\n",
       "      <td>0.713658</td>\n",
       "      <td>-0.913156</td>\n",
       "      <td>-0.752478</td>\n",
       "      <td>-1.270842</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.581969</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>-0.706239</td>\n",
       "      <td>-1.033836</td>\n",
       "      <td>0.101179</td>\n",
       "      <td>-0.227085</td>\n",
       "      <td>-0.489536</td>\n",
       "      <td>0.298171</td>\n",
       "      <td>0.019837</td>\n",
       "      <td>-0.972457</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.581969</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>-1.092008</td>\n",
       "      <td>-0.283250</td>\n",
       "      <td>0.786192</td>\n",
       "      <td>0.199805</td>\n",
       "      <td>-0.327473</td>\n",
       "      <td>0.701946</td>\n",
       "      <td>1.704888</td>\n",
       "      <td>0.106243</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.581969</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>-0.786816</td>\n",
       "      <td>-0.860624</td>\n",
       "      <td>0.169681</td>\n",
       "      <td>0.032635</td>\n",
       "      <td>-0.878487</td>\n",
       "      <td>-0.509380</td>\n",
       "      <td>-0.717373</td>\n",
       "      <td>-1.555508</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1.718304</td>\n",
       "      <td>-1.718304</td>\n",
       "      <td>-1.431425</td>\n",
       "      <td>0.409598</td>\n",
       "      <td>-0.241327</td>\n",
       "      <td>-0.703237</td>\n",
       "      <td>0.835840</td>\n",
       "      <td>-0.408436</td>\n",
       "      <td>-1.180866</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.581969</td>\n",
       "      <td>0.581969</td>\n",
       "      <td>0.180098</td>\n",
       "      <td>0.063174</td>\n",
       "      <td>-0.515332</td>\n",
       "      <td>-0.724881</td>\n",
       "      <td>-1.235025</td>\n",
       "      <td>-0.004661</td>\n",
       "      <td>-0.822689</td>\n",
       "      <td>-1.871042</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type_r    type_w  fixed acidity  volatile acidity  citric acid  \\\n",
       "0    1.718304 -1.718304       0.663555          1.448871    -0.583833   \n",
       "1   -0.581969  0.581969      -0.867392          0.544211    -0.507574   \n",
       "2   -0.581969  0.581969       0.905283         -0.860624     2.978233   \n",
       "3   -0.581969  0.581969       1.388740         -0.456462     0.238182   \n",
       "4   -0.581969  0.581969      -0.464511         -1.033836    -0.035823   \n",
       "..        ...       ...            ...               ...          ...   \n",
       "995 -0.581969  0.581969      -0.706239         -1.033836     0.101179   \n",
       "996 -0.581969  0.581969      -1.092008         -0.283250     0.786192   \n",
       "997 -0.581969  0.581969      -0.786816         -0.860624     0.169681   \n",
       "998  1.718304 -1.718304      -1.431425          0.409598    -0.241327   \n",
       "999 -0.581969  0.581969       0.180098          0.063174    -0.515332   \n",
       "\n",
       "     residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  \\\n",
       "0         -0.568905   0.806965            -0.307492             -0.190794   \n",
       "1         -0.811454  -0.619186             0.348643              0.142705   \n",
       "2         -0.172977  -0.359886             0.336280              1.384973   \n",
       "3          1.179730  -0.846074            -0.585088              0.879915   \n",
       "4         -0.898027   0.713658            -0.913156             -0.752478   \n",
       "..              ...        ...                  ...                   ...   \n",
       "995       -0.227085  -0.489536             0.298171              0.019837   \n",
       "996        0.199805  -0.327473             0.701946              1.704888   \n",
       "997        0.032635  -0.878487            -0.509380             -0.717373   \n",
       "998       -0.703237   0.835840            -0.408436             -1.180866   \n",
       "999       -0.724881  -1.235025            -0.004661             -0.822689   \n",
       "\n",
       "      density  ...  citric acid_missing  residual sugar_missing  \\\n",
       "0    0.505750  ...                    0                       1   \n",
       "1   -1.665259  ...                    1                       0   \n",
       "2   -0.773533  ...                    0                       0   \n",
       "3    0.697814  ...                    0                       0   \n",
       "4   -1.270842  ...                    0                       0   \n",
       "..        ...  ...                  ...                     ...   \n",
       "995 -0.972457  ...                    0                       0   \n",
       "996  0.106243  ...                    0                       1   \n",
       "997 -1.555508  ...                    0                       0   \n",
       "998  0.018731  ...                    0                       0   \n",
       "999 -1.871042  ...                    0                       0   \n",
       "\n",
       "     chlorides_missing  free sulfur dioxide_missing  \\\n",
       "0                    0                            0   \n",
       "1                    0                            0   \n",
       "2                    0                            1   \n",
       "3                    0                            0   \n",
       "4                    1                            0   \n",
       "..                 ...                          ...   \n",
       "995                  0                            0   \n",
       "996                  0                            0   \n",
       "997                  0                            0   \n",
       "998                  1                            0   \n",
       "999                  0                            0   \n",
       "\n",
       "     total sulfur dioxide_missing  density_missing  pH_missing  \\\n",
       "0                               0                0           0   \n",
       "1                               0                0           0   \n",
       "2                               1                0           0   \n",
       "3                               0                0           0   \n",
       "4                               0                0           0   \n",
       "..                            ...              ...         ...   \n",
       "995                             0                0           0   \n",
       "996                             0                1           0   \n",
       "997                             0                0           0   \n",
       "998                             1                0           0   \n",
       "999                             0                0           0   \n",
       "\n",
       "     sulphates_missing  alcohol_missing  type_missing  \n",
       "0                    0                0             0  \n",
       "1                    0                0             0  \n",
       "2                    0                0             0  \n",
       "3                    0                0             0  \n",
       "4                    0                0             0  \n",
       "..                 ...              ...           ...  \n",
       "995                  0                0             0  \n",
       "996                  0                0             0  \n",
       "997                  0                0             0  \n",
       "998                  0                1             0  \n",
       "999                  0                0             0  \n",
       "\n",
       "[1000 rows x 25 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{purple}{\\text{Conclusion}}$\n",
    "\n",
    "\n",
    "* The bulk of work with models dealing with missing data uses decision tree derivative models\n",
    "* Neural Networks can be used for imputation\n",
    "* Several strategies for integrating imputation into model building pipelines\n",
    "  * Imputer should be a processing step (important that models are saveable)\n",
    "  * Multiple Imputation can use data augmentation or multiple models\n",
    "  * Bagging can be applied to single imputation performed multiply\n",
    "  * Missingness (or imputed) can be added as a flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{purple}{\\text{References}}$\n",
    "* Gondara, L., Wang, K.: Mida: Multiple imputation using denoising autoencoders.\n",
    "In: Phung, D., Tseng, V.S., Webb, G.I., Ho, B., Ganji, M., Rashidi, L. (eds.)\n",
    "_Advances in Knowledge Discovery and Data Mining_. pp. 260–272. Springer International Publishing, Cham (2018)\n",
    "* Jiang, W., Josse, J., Lavielle, M: Logistic regression with missing covariates–parameter estimation, model selection and prediction. _Computational and Statistics Analysis_, 2019.\n",
    "* Lu, H.-m., Perrone, G., & Unpingco, J.: Multiple imputation with denoising autoencoder using metamorphic truth and imputation feedback, _Machine Learning and Data Mining in Pattern Recognition, 16th International\n",
    "Conference on Machine Learning and Data Mining, MLDM 2020_,Amsterdam, The Netherlands, July 20-21, 2020, Proceedings,\n",
    "pages 197–208.\n",
    "* Perez-Lebel, A., Varoquaux, G., Le Morvan, M., Josse, J., Poline, J.-B.: Benchmarking missing-values approaches for predictive models on health databases, _GigaScience_, Volume 11, 2022, https://doi.org/10.1093/gigascience/giac013\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "authorship_tag": "ABX9TyPpLv6R3L0XkmwitYmu11KS",
   "collapsed_sections": [],
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
